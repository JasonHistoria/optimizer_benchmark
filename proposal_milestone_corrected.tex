\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=0.8in,a4paper]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{pifont}
\usepackage{array} % For better table formatting

% Compact formatting
\setlist{nosep,leftmargin=*}
\setlength{\parskip}{1pt}
\setlength{\parindent}{0pt}

% Custom symbols
\newcommand{\cmark}{\ding{51}}  % ✓
\newcommand{\xmark}{\ding{55}}  % ✗

\title{CSE 493S Project Proposal: \\
\large Systematic Comparison of Adam-based Optimization Algorithms with Theoretical Analysis}
\author{
    Jinghao Liu (jliu63) \and
    Xuan Zhang (xuanz24) \and
    Yuzheng Zhang (yuzhez4)
}
\date{October 17, 2024 -- Updated November 6, 2024}

\begin{document}
\maketitle
\vspace{-8mm}

\section*{Team \& Project Direction}
\textbf{Team:} Jinghao Liu (jliu63), Xuan Zhang (xuanz24), Yuzheng Zhang (yuzhez4). All are undergraduate students.  
\textbf{Direction:} This project follows the \textbf{fair comparisons for algorithms} direction with a theoretical component. We will systematically compare five optimization algorithms (SGD with momentum, Adam, AdamW, RAdam, and Lion) and analyze their mathematical foundations.

\section*{Core Citations}
\begin{enumerate}
    \item Kingma \& Ba (2014), \textit{Adam: A method for stochastic optimization}, \href{https://arxiv.org/abs/1412.6980}{arXiv:1412.6980}.
    \item Loshchilov \& Hutter (2019), \textit{Decoupled weight decay regularization}, \href{https://arxiv.org/abs/1711.05101}{ICLR}.
    \item Liu et al. (2020), \textit{On the variance of the adaptive learning rate and beyond}, \href{https://arxiv.org/abs/1908.03265}{ICLR}.
    \item Chen et al. (2023), \textit{Symbolic discovery of optimization algorithms}, \href{https://arxiv.org/abs/2302.06675}{arXiv:2302.06675}.
\end{enumerate}

\section*{Goals, Deliverables \& First Milestone (by Nov 6)}
\textbf{Expected Deliverables:}
\begin{itemize}
    \item Comparison curves (training loss, validation accuracy) on CIFAR-10 \& CIFAR-100.
    \item Metrics table (accuracy, speed, stability) and update-rule analysis.
    \item Empirical validation of theoretical insights; public GitHub repo.
\end{itemize}
\textbf{First Milestone:} Complete CIFAR-10 experiments (5 optimizers $\times$ 3 seeds), partial CIFAR-100 runs, unified training pipeline, and preliminary theoretical validation.

\section*{Methodology \& Feasibility}
\textbf{Empirical Setup:} CIFAR-10 \& CIFAR-100 (ResNet-18) via PyTorch; using \texttt{torch.optim} for SGD/Adam/AdamW/RAdam and \href{https://github.com/lucidrains/lion-pytorch}{lion-pytorch} for Lion.  
\textbf{Computational Feasibility:} $\sim$11h main experiments + $\sim$3h sensitivity on Google Colab GPUs, distributed across team members (3 $\times$ 5h each).

\section*{Theoretical Focus}
We hypothesize that: (1) RAdam's variance rectification enhances early-epoch stability; (2) AdamW's decoupled weight decay reduces the train--test gap; (3) Lion's sign-based updates improve robustness to label noise. These will be analyzed mathematically and verified empirically.

\newpage
\section*{Milestone Update (November 6, 2024)}

\subsection*{Completed Milestones}

\cmark~\textbf{Theoretical framework:} Update rule derivations completed for all 5 optimizers; 3 testable hypotheses documented with mathematical justification

\cmark~\textbf{Complete CIFAR-10 experiments:} All 5 optimizers $\times$ 3 seeds = 15 experiments completed with results tables and comparison plots

\cmark~\textbf{Working pipeline:} Unified training framework with automated logging, reproducible random seeds, and visualization scripts

\cmark~\textbf{Preliminary analysis:} Initial hypothesis validation completed on CIFAR-10 data

\xmark~\textbf{Partial CIFAR-100 results:} 1 optimizer completed; remaining experiments in progress due to compute constraints. The structure of ResNet-18 may cause overfitting on CIFAR-100, so we are considering using a different architecture.

\subsection*{Key Empirical Results}

\subsubsection*{CIFAR-10 Performance (ResNet-18, 100 epochs)}

\begin{center}
\begin{tabular}{lcccc}
\toprule
\textbf{Optimizer} & \textbf{Accuracy} & \textbf{Std Dev} & \textbf{Conv.} & \textbf{Notes} \\
\midrule
SGD + Momentum & 87.49\% & $\pm$0.13\% & Slow start & Best final \\
AdamW & 86.31\% & $\pm$0.14\% & Fast & Better gen. \\
Adam & 86.03\% & $\pm$0.10\% & Fast & Baseline \\
Lion & 85.77\% & $\pm$0.06\% & Fast & Most stable \\
RAdam & 84.66\% & $\pm$0.37\% & Medium & High var. \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Key Findings:}
\begin{itemize}
    \item \textbf{SGD outperforms Adam by 1.46\%} despite slower early convergence
    \item \textbf{AdamW $>$ Adam} by 0.28\%, validating decoupled weight decay benefit
    \item \textbf{Lion most stable} ($\pm$0.06\%) but with final accuracy trade-off
    \item \textbf{RAdam unexpectedly unstable} ($\pm$0.37\%), contradicting theory
\end{itemize}

\subsection*{Design Choices}

\textbf{Fair Experimental Protocol:} Fixed hyperparameters across all optimizers (lr=0.001 for adaptive methods, lr=0.1 for SGD; momentum=0.9, $\beta_1$=0.9, $\beta_2$=0.999), multiple seeds (3) per optimizer, identical data augmentation and batch size (128), automated metric logging.

\textbf{Hardware-Agnostic Implementation:} Supports CUDA, Apple Silicon (MPS), and CPU for distributed team experimentation.

\textbf{Beyond-Proposal Innovation:} Developed layer-wise gradient tracking framework to understand optimizer differences mechanistically. Ready for analysis in next phase.

\subsection*{Challenges \& Mitigation}

\begin{center}
\begin{tabular}{p{3.5cm}p{2cm}p{6.5cm}}
\toprule
\textbf{Challenge} & \textbf{Status} & \textbf{Solution} \\
\midrule
CIFAR-100 computation cost & In progress & Using NVIDIA RTX 3060; prioritizing comprehensive CIFAR-10 analysis \\
\midrule
RAdam instability & Investigating & Running 200-epoch training; analyzing early vs. late training dynamics \\
\midrule
Theory documentation & On track & Derivations complete; synthesis planned for final report \\
\bottomrule
\end{tabular}
\end{center}

\subsection*{Updated Hypotheses}

\textbf{H1 (RAdam stability):} REVISED -- RAdam shows \textit{higher} variance ($\pm$0.37\%) than expected. Investigating if stability benefits emerge only in specific scenarios or are dataset-dependent.

\textbf{H2 (AdamW regularization):} SUPPORTED -- AdamW (86.31\%) outperforms Adam (86.03\%), consistent with theoretical expectation of better generalization.

\textbf{H3 (Lion robustness):} PARTIALLY VALIDATED -- Lion demonstrates lowest variance ($\pm$0.06\%) across seeds, confirming sign-based robustness. Pending label-noise validation.

\subsection*{Resource Usage \& Timeline}

\textbf{GPU Hours:}
\begin{itemize}
    \item CIFAR-10: 15 experiments $\approx$ 5 GPU hours \cmark
    \item CIFAR-100: 2/5 optimizers $\approx$ 2 GPU hours, $\sim$3 hours remaining
    \item Total so far: $\approx$ 7 GPU hours (within Colab free tier + local GPU)
\end{itemize}

\textbf{Revised Timeline:}
\begin{itemize}
    \item Nov 7--15: Complete CIFAR-100; validate remaining hypotheses
    \item Nov 16--25: Hyperparameter sensitivity; theoretical synthesis
    \item Nov 26--30: Final report writing and polishing
    \item Dec 1--3: Final submission and presentation preparation
\end{itemize}

\subsection*{Addressing Feedback}

\textbf{On ``Why default to Adam?''} Our CIFAR-10 results show SGD achieves 1.46\% higher accuracy than Adam, challenging this common practice. We propose nuanced guidelines: use Adam for rapid prototyping and unstable gradients; use SGD for final training when compute allows tuning; use AdamW when generalization is critical.

\textbf{On ``Conditions for different optimizers''} (Jamie's comment): Our gradient tracking framework will reveal layer-wise patterns showing when each optimizer excels. Preliminary insights suggest Adam-family optimizers show higher gradient variance in early layers, which may explain CNN performance differences.

\subsection*{Conclusion}

We have met all core milestone objectives for CIFAR-10. The results provide strong empirical evidence challenging the conventional practice of defaulting to Adam, showing SGD can achieve substantially higher accuracy with proper tuning. The unexpected instability of RAdam reveals an important theory-practice gap worthy of investigation. We are on track to deliver a comprehensive analysis by the final deadline.

\end{document}

