{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer Comparison Analysis\n",
    "## CSE 493S Project - Results Analysis and Visualization\n",
    "\n",
    "**Team Members:** Jinghao Liu, Xuan Zhang, Yuzheng Zhang\n",
    "\n",
    "This notebook analyzes and visualizes results from multiple optimizer experiments.\n",
    "\n",
    "**Contents:**\n",
    "1. Load and compare results from multiple experiments\n",
    "2. Generate comparison plots\n",
    "3. Statistical analysis\n",
    "4. Hypothesis testing (H1, H2, H3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"\u2713 Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(results_dir='results', dataset='cifar10'):\n",
    "    \"\"\"Load all experiment results for a dataset.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    pattern = f\"{results_dir}/{dataset}_*_metrics.json\"\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    print(f\"Found {len(files)} experiment results for {dataset.upper()}\")\n",
    "    \n",
    "    for file in files:\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        optimizer = data['config']['optimizer']\n",
    "        seed = data['config']['seed']\n",
    "        \n",
    "        if optimizer not in results:\n",
    "            results[optimizer] = []\n",
    "        \n",
    "        results[optimizer].append({\n",
    "            'seed': seed,\n",
    "            'history': data['history'],\n",
    "            'best_accuracy': data['best_accuracy'],\n",
    "            'final_accuracy': data['final_accuracy'],\n",
    "            'config': data['config']\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load results\n",
    "cifar10_results = load_results('results', 'cifar10')\n",
    "cifar100_results = load_results('results', 'cifar100')\n",
    "\n",
    "print(f\"\\nCIFAR-10 optimizers: {list(cifar10_results.keys())}\")\n",
    "print(f\"CIFAR-100 optimizers: {list(cifar100_results.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistics(results):\n",
    "    \"\"\"Compute mean and std for each optimizer.\"\"\"\n",
    "    stats = []\n",
    "    \n",
    "    for optimizer, runs in results.items():\n",
    "        if not runs:\n",
    "            continue\n",
    "        \n",
    "        best_accs = [r['best_accuracy'] for r in runs]\n",
    "        final_accs = [r['final_accuracy'] for r in runs]\n",
    "        \n",
    "        stats.append({\n",
    "            'Optimizer': optimizer.upper(),\n",
    "            'Runs': len(runs),\n",
    "            'Best Acc (%)': f\"{np.mean(best_accs):.2f} \u00b1 {np.std(best_accs):.2f}\",\n",
    "            'Final Acc (%)': f\"{np.mean(final_accs):.2f} \u00b1 {np.std(final_accs):.2f}\",\n",
    "            'Best (mean)': np.mean(best_accs),\n",
    "            'Best (std)': np.std(best_accs)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "# CIFAR-10 statistics\n",
    "if cifar10_results:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CIFAR-10 Results\")\n",
    "    print(\"=\" * 80)\n",
    "    df_c10 = compute_statistics(cifar10_results)\n",
    "    print(df_c10[['Optimizer', 'Runs', 'Best Acc (%)', 'Final Acc (%)']].to_string(index=False))\n",
    "\n",
    "# CIFAR-100 statistics\n",
    "if cifar100_results:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CIFAR-100 Results\")\n",
    "    print(\"=\" * 80)\n",
    "    df_c100 = compute_statistics(cifar100_results)\n",
    "    print(df_c100[['Optimizer', 'Runs', 'Best Acc (%)', 'Final Acc (%)']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(results, dataset_name, save=True):\n",
    "    \"\"\"Create comprehensive comparison plots.\"\"\"\n",
    "    if not results:\n",
    "        print(f\"No results found for {dataset_name}\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    colors = sns.color_palette(\"husl\", len(results))\n",
    "    \n",
    "    # Plot 1: Training Loss\n",
    "    ax = axes[0, 0]\n",
    "    for (opt_name, runs), color in zip(results.items(), colors):\n",
    "        losses = [r['history']['train_loss'] for r in runs]\n",
    "        max_len = max(len(l) for l in losses)\n",
    "        \n",
    "        # Pad sequences\n",
    "        padded = [l + [l[-1]] * (max_len - len(l)) for l in losses]\n",
    "        losses_array = np.array(padded)\n",
    "        \n",
    "        mean_loss = np.mean(losses_array, axis=0)\n",
    "        std_loss = np.std(losses_array, axis=0)\n",
    "        epochs = range(1, len(mean_loss) + 1)\n",
    "        \n",
    "        ax.plot(epochs, mean_loss, label=opt_name.upper(), linewidth=2, color=color)\n",
    "        ax.fill_between(epochs, mean_loss - std_loss, mean_loss + std_loss, alpha=0.2, color=color)\n",
    "    \n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Training Loss')\n",
    "    ax.set_title('Training Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Test Accuracy\n",
    "    ax = axes[0, 1]\n",
    "    for (opt_name, runs), color in zip(results.items(), colors):\n",
    "        accs = [r['history']['test_acc'] for r in runs]\n",
    "        max_len = max(len(a) for a in accs)\n",
    "        padded = [a + [a[-1]] * (max_len - len(a)) for a in accs]\n",
    "        accs_array = np.array(padded)\n",
    "        \n",
    "        mean_acc = np.mean(accs_array, axis=0)\n",
    "        std_acc = np.std(accs_array, axis=0)\n",
    "        epochs = range(1, len(mean_acc) + 1)\n",
    "        \n",
    "        ax.plot(epochs, mean_acc, label=opt_name.upper(), linewidth=2, color=color)\n",
    "        ax.fill_between(epochs, mean_acc - std_acc, mean_acc + std_acc, alpha=0.2, color=color)\n",
    "    \n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Test Accuracy (%)')\n",
    "    ax.set_title('Test Accuracy')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Final Accuracy Bar Chart\n",
    "    ax = axes[1, 0]\n",
    "    optimizers = list(results.keys())\n",
    "    final_means = [np.mean([r['final_accuracy'] for r in results[opt]]) for opt in optimizers]\n",
    "    final_stds = [np.std([r['final_accuracy'] for r in results[opt]]) for opt in optimizers]\n",
    "    \n",
    "    x_pos = np.arange(len(optimizers))\n",
    "    ax.bar(x_pos, final_means, yerr=final_stds, capsize=5, alpha=0.7, color=colors)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels([opt.upper() for opt in optimizers])\n",
    "    ax.set_ylabel('Test Accuracy (%)')\n",
    "    ax.set_title('Final Test Accuracy')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for i, (mean, std) in enumerate(zip(final_means, final_stds)):\n",
    "        ax.text(i, mean + std + 0.5, f'{mean:.2f}\u00b1{std:.2f}', ha='center', fontsize=9)\n",
    "    \n",
    "    # Plot 4: Best Accuracy Bar Chart\n",
    "    ax = axes[1, 1]\n",
    "    best_means = [np.mean([r['best_accuracy'] for r in results[opt]]) for opt in optimizers]\n",
    "    best_stds = [np.std([r['best_accuracy'] for r in results[opt]]) for opt in optimizers]\n",
    "    \n",
    "    ax.bar(x_pos, best_means, yerr=best_stds, capsize=5, alpha=0.7, color=colors)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels([opt.upper() for opt in optimizers])\n",
    "    ax.set_ylabel('Test Accuracy (%)')\n",
    "    ax.set_title('Best Test Accuracy')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for i, (mean, std) in enumerate(zip(best_means, best_stds)):\n",
    "        ax.text(i, mean + std + 0.5, f'{mean:.2f}\u00b1{std:.2f}', ha='center', fontsize=9)\n",
    "    \n",
    "    plt.suptitle(f'Optimizer Comparison on {dataset_name}', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(f'{dataset_name}_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"\u2713 Plot saved as {dataset_name}_comparison.png\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Generate plots\n",
    "plot_comparison(cifar10_results, 'CIFAR-10')\n",
    "plot_comparison(cifar100_results, 'CIFAR-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "### H1: RAdam Early Training Stability\n",
    "\n",
    "Test if RAdam has lower variance in the first 10 epochs compared to Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_h1_early_stability(results):\n",
    "    \"\"\"Test H1: RAdam early training stability.\"\"\"\n",
    "    if 'adam' not in results or 'radam' not in results:\n",
    "        print(\"\u26a0\ufe0f  Need both Adam and RAdam results for H1\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"H1: RAdam Early Training Stability (First 10 Epochs)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Get losses for first 10 epochs\n",
    "    adam_losses = [r['history']['train_loss'][:10] for r in results['adam']]\n",
    "    radam_losses = [r['history']['train_loss'][:10] for r in results['radam']]\n",
    "    \n",
    "    # Calculate coefficient of variation\n",
    "    adam_cv = np.mean([np.std(l) / np.mean(l) for l in adam_losses])\n",
    "    radam_cv = np.mean([np.std(l) / np.mean(l) for l in radam_losses])\n",
    "    \n",
    "    reduction = (adam_cv - radam_cv) / adam_cv * 100\n",
    "    \n",
    "    print(f\"Adam CV:  {adam_cv:.4f}\")\n",
    "    print(f\"RAdam CV: {radam_cv:.4f}\")\n",
    "    print(f\"Reduction: {reduction:.1f}%\")\n",
    "    \n",
    "    if reduction >= 20:\n",
    "        print(\"\\n\u2713 H1 SUPPORTED: RAdam reduces variance by \u226520%\")\n",
    "    else:\n",
    "        print(f\"\\n\u2717 H1 NOT SUPPORTED: Reduction is {reduction:.1f}% < 20%\")\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    for losses in adam_losses:\n",
    "        plt.plot(range(1, 11), losses, 'o-', alpha=0.5, color='blue')\n",
    "    for losses in radam_losses:\n",
    "        plt.plot(range(1, 11), losses, 's-', alpha=0.5, color='red')\n",
    "    \n",
    "    plt.plot([], [], 'o-', color='blue', label='Adam', linewidth=2)\n",
    "    plt.plot([], [], 's-', color='red', label='RAdam', linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.title('H1: Early Training Stability Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('H1_early_stability.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "test_h1_early_stability(cifar100_results if cifar100_results else cifar10_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H2: AdamW Regularization Effect\n",
    "\n",
    "Test if AdamW shows smaller train-test gap than Adam with high weight decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_h2_regularization(results):\n",
    "    \"\"\"Test H2: AdamW regularization effect.\"\"\"\n",
    "    if 'adam' not in results or 'adamw' not in results:\n",
    "        print(\"\u26a0\ufe0f  Need both Adam and AdamW results for H2\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"H2: AdamW Regularization Effect\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    def compute_gap(runs):\n",
    "        gaps = []\n",
    "        for r in runs:\n",
    "            train_acc = r['history']['train_acc'][-1]\n",
    "            test_acc = r['history']['test_acc'][-1]\n",
    "            gaps.append(train_acc - test_acc)\n",
    "        return np.mean(gaps), np.std(gaps)\n",
    "    \n",
    "    adam_gap_mean, adam_gap_std = compute_gap(results['adam'])\n",
    "    adamw_gap_mean, adamw_gap_std = compute_gap(results['adamw'])\n",
    "    \n",
    "    print(f\"Adam gap:  {adam_gap_mean:.2f} \u00b1 {adam_gap_std:.2f}%\")\n",
    "    print(f\"AdamW gap: {adamw_gap_mean:.2f} \u00b1 {adamw_gap_std:.2f}%\")\n",
    "    print(f\"Reduction: {adam_gap_mean - adamw_gap_mean:.2f}%\")\n",
    "    \n",
    "    if adamw_gap_mean < adam_gap_mean:\n",
    "        print(\"\\n\u2713 H2 SUPPORTED: AdamW has smaller train-test gap\")\n",
    "    else:\n",
    "        print(\"\\n\u2717 H2 NOT SUPPORTED: AdamW gap not smaller\")\n",
    "\n",
    "test_h2_regularization(cifar10_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H3: Lion Robustness to Label Noise\n",
    "\n",
    "Test if Lion maintains better accuracy than Adam under label noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_h3_noise_robustness():\n",
    "    \"\"\"Test H3: Lion robustness to label noise.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"H3: Lion Robustness to Label Noise\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nTo test H3, run experiments with label_noise=0.2:\")\n",
    "    print(\"  python src/train.py --optimizer adam --label-noise 0.2 --seed 42\")\n",
    "    print(\"  python src/train.py --optimizer lion --label-noise 0.2 --seed 42\")\n",
    "    print(\"\\nThen load results and compare final accuracies.\")\n",
    "\n",
    "test_h3_noise_robustness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_convergence(results, target_acc=90.0):\n",
    "    \"\"\"Analyze convergence speed (epochs to reach target accuracy).\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"Convergence Speed (Epochs to reach {target_acc}% accuracy)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for opt_name, runs in results.items():\n",
    "        epochs_to_target = []\n",
    "        \n",
    "        for r in runs:\n",
    "            test_accs = r['history']['test_acc']\n",
    "            for i, acc in enumerate(test_accs):\n",
    "                if acc >= target_acc:\n",
    "                    epochs_to_target.append(i + 1)\n",
    "                    break\n",
    "            else:\n",
    "                epochs_to_target.append(len(test_accs))  # Never reached\n",
    "        \n",
    "        mean_epochs = np.mean(epochs_to_target)\n",
    "        std_epochs = np.std(epochs_to_target)\n",
    "        \n",
    "        print(f\"{opt_name.upper():10s}: {mean_epochs:.1f} \u00b1 {std_epochs:.1f} epochs\")\n",
    "\n",
    "if cifar10_results:\n",
    "    analyze_convergence(cifar10_results, target_acc=90.0)\n",
    "\n",
    "if cifar100_results:\n",
    "    analyze_convergence(cifar100_results, target_acc=70.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LaTeX table for report\n",
    "if cifar10_results:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"LaTeX Table (CIFAR-10)\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    print(df_c10[['Optimizer', 'Runs', 'Best Acc (%)', 'Final Acc (%)']].to_latex(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "if cifar10_results:\n",
    "    df_c10.to_csv('cifar10_summary.csv', index=False)\n",
    "    print(\"\\n\u2713 CIFAR-10 summary saved to cifar10_summary.csv\")\n",
    "\n",
    "if cifar100_results:\n",
    "    df_c100.to_csv('cifar100_summary.csv', index=False)\n",
    "    print(\"\u2713 CIFAR-100 summary saved to cifar100_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides:\n",
    "1. Statistical summary of all experiments\n",
    "2. Comprehensive comparison plots\n",
    "3. Hypothesis testing (H1, H2, H3)\n",
    "4. Convergence analysis\n",
    "5. Export capabilities for reports\n",
    "\n",
    "**For your report:**\n",
    "- Use the comparison plots\n",
    "- Include the summary statistics table\n",
    "- Report hypothesis testing results\n",
    "- Analyze convergence speed differences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}